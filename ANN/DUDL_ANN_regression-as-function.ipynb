{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "DUDL_ANN_regression.ipynb",
   "provenance": [
    {
     "file_id": "1FtQ99beHYcDFDywLdaPgFm-KjBeI8PvD",
     "timestamp": 1615884593383
    }
   ],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyOfbw74CIER4uMxvaGa9RbX"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning - CORRECT SOLUTION in\n",
    "## SECTION: ANNs\n",
    "### LECTURE: ANN for regression\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j7-LiwqUMGYL"
   },
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j-SP8NPsMNRL"
   },
   "source": [
    "# create data\n",
    "global N\n",
    "\n",
    "def create_data(slope:float ) -> dict:\n",
    "    global x, y, losses, epochs\n",
    "    N = 50\n",
    "    x = torch.randn(N, 1)\n",
    "    y = slope * x + torch.randn(N, 1) / 2\n",
    "\n",
    "    data = {}\n",
    "    data[\"input\"] = x\n",
    "    data[\"output\"] = y\n",
    "\n",
    "    return data\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "krQeh5wYMNla"
   },
   "source": [
    "# build model\n",
    "def build_model() -> nn.Sequential:\n",
    "    # build model\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(1, 1),  # input layer\n",
    "        nn.ReLU(),  # activation function\n",
    "        nn.Linear(1, 1)  # output layer\n",
    "    )\n",
    "    return model\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "of9E8ClxMNsD"
   },
   "source": [
    "# train the model\n",
    "def train_model(data: dict, model:nn.Sequential) -> nn.Sequential:\n",
    "\n",
    "    # and plot\n",
    "    # plt.plot(x,y,'s')\n",
    "    # plt.show()\n",
    "\n",
    "    # learning rate\n",
    "    learning_rate = .05\n",
    "    # loss function\n",
    "    loss_function = nn.MSELoss()\n",
    "    # optimizer (the flavor of gradient descent to implement)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # train the model\n",
    "    epochs = 500\n",
    "    ## Train the model!\n",
    "    loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        # forward pass\n",
    "        yHat = model(data[\"input\"])\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_function(yHat, data[\"output\"])\n",
    "        accuracy = (yHat/data[\"output\"]).mean()\n",
    "\n",
    "        # save the loss\n",
    "        # losses[epoch] = loss\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss, accuracy\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Driver code\n",
    "model = build_model()\n",
    "\n",
    "# run the experiment(s)\n",
    "epochs = 500\n",
    "slopes = np.linspace(-2,2,num=21)\n",
    "losses = torch.zeros(epochs)\n",
    "accuracies = torch.zeros(epochs)\n",
    "\n",
    "for i, slope in enumerate(slopes, 0):\n",
    "    data = create_data(slope + 0.0)\n",
    "    loss,accuracy = train_model(data=data, model=model)\n",
    "    losses[i] = loss\n",
    "    accuracies[i] = accuracy\n",
    "# show the losses as a function of the slope\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i1TCt0mpMNxC"
   },
   "source": [
    "# plot the data\n",
    "plt.plot(slopes, losses.detach())\n",
    "plt.xlabel('Slopes')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vGkQUzqhUFpq"
   },
   "source": [
    "plt.plot(slopes, accuracies.detach())\n",
    "plt.xlabel('Slopes')\n",
    "plt.ylabel('Accuracies')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmraVzTcJ0x1"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pml6nCTcAMWC"
   },
   "source": [
    "# 1) How much data is \"enough\"? Try different values of N and see how low the loss gets. \n",
    "#    Do you still get low loss (\"low\" is subjective, but let's say loss<.25) with N=10? N=5?\n",
    "# \n",
    "# 2) Does your conclusion above depend on the amount of noise in the data? Try changing the noise level\n",
    "#    by changing the division (\"/2\") when creating y as x+randn.\n",
    "# \n",
    "# 3) Notice that the model doesn't always work well. Put the original code (that is, N=30 and /2 noise)\n",
    "#    into a function or a for-loop and repeat the training 100 times (each time using a fresh model instance).\n",
    "#    Then count the number of times the model had a loss>.25."
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}
